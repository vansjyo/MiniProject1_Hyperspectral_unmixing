@article{Captions2016,
author = {Captions, Their and Reading, Further and Index, Subject},
file = {:D$\backslash$:/LaTex/latex{\_}author{\_}rtip2r2018/authinst.pdf:pdf},
pages = {1--13},
title = {{A T X 2 SVProc Document Class L E $\epsilon$ Author Instructions for Step-by-Step Instructions}},
year = {2016}
}
@article{Author,
abstract = {The abstract is a mandatory element that should summarize the con-tents of the paper and should contain 15-250 words. Abstract and keywords are made freely available in SpringerLink.},
author = {Author, First and Author, Second and Author, Third and Author, Fourth},
file = {:D$\backslash$:/LaTex/latex{\_}author{\_}rtip2r2018/Springer{\_}Guidelines{\_}for{\_}Authors{\_}of{\_}Proceedings.pdf:pdf},
keywords = {by,capitalized,each keyword should be,here,if possible,middots,please list your keywords,the first letter of,they should be separated},
pages = {1--11},
title = {{Springer Guidelines for Authors of Proceedings}}
}
@article{Luxburg2006,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. Nevertheless, on the first glance spectral clustering looks a bit mysterious, and it is not obvious to see why it works at all and what it really does. This article is a tutorial introduction to spectral clustering. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Luxburg, Ulrike Von},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
file = {:C$\backslash$:/Users/deric/Downloads/Luxburg06{\_}TR.pdf:pdf},
isbn = {0960-3174},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {graph laplacian,spectral clustering},
number = {March},
pages = {395--416},
pmid = {19784854},
title = {{A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering}},
url = {http://www.springerlink.com/index/10.1007/s11222-007-9033-z},
volume = {17},
year = {2006}
}
@article{Ng2002,
abstract = {Despite many empirical successes of spectral clustering methods -- algorithms that clusters points using eigenvectors of matrices derived from the data -- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
doi = {10.1.1.19.8100},
file = {:C$\backslash$:/Users/deric/Downloads/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf:pdf},
isbn = {0818619155},
issn = {{\textless}null{\textgreater}},
journal = {Advances in Neural Information Processing Systems 14},
pages = {849--856},
title = {{On Spectral Clustering: Analysis and an Algorithm}},
url = {http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf},
year = {2002}
}
@article{Honarkhah2011,
abstract = {The advent of multiple-point geostatistics (MPS) gave rise to the integration of com- plex subsurface geological structures and features into the model by the concept of training images. Initial algorithms generate geologically realistic realizations by us- ing these training images to obtain conditional probabilities needed in a stochastic simulation framework. More recent pattern-based geostatistical algorithms attempt to improve the accuracy of the training image pattern reproduction. In these ap- proaches, the training image is used to construct a pattern database. Consequently, sequential simulation will be carried out by selecting a pattern from the database and pasting it onto the simulation grid. One of the shortcomings of the present algorithms is the lack of a unifying framework for classifying and modeling the patterns from the training image. In this thesis an entirely different approach will be taken towards geostatistical modeling. A novel, principled and unified technique for pattern analysis and generation that ensures computational efficiency and enables a straightforward incorporation of domain knowledge will be presented. In the developed methodology (called DisPAT), patterns scanned from the training image are represented as points in a Cartesian space using multi-dimensional scaling. The idea behind this mapping is to use distance functions as a tool for analyzing variability between all the patterns in a training image. These distance functions can be tailored to the application at hand. Next, by significantly reducing the dimensionality of the problem and using kernel space mapping, an improved pattern classification algorithm is obtained. Additionally, a multi-resolution approach is presented for modeling the patterns of the training image at various scales. The proposed distance-based pattern-modeling techniques are inspired by biology and the human visual system. Several examples are presented and a qualitative comparison is made with previous methods to demonstrate the capabilities of this simple, yet powerful system. We show how the proposed methodology is much less sensitive to the user-provided parameters, and at the same time has the potential to reduce computational time significantly. Another aspect of present MPS algorithms is their strong dependence on the algorithmic parameters that the practitioner specifies. This, not only entails tedious trial-and-error computations for tuning the parameters, but also triggers potential subjectivity in modeler's decisions. In order to obtain a systematic pattern-based approach, new techniques on learning the optimal set of parameters are introduced. A series of examples is provided to verify the competency of these approaches in, not only facilitating the modeling process, but also ensuring a rigorous simulation framework. Furthermore, better data conditioning algorithms for both the hard data and the soft data are proposed. An improved pattern continuity and data-conditioning capability is observed in the generated realizations for both continuous and categorical variables. Some improvements in multi-scale data conditioning are also described. Overall, we demonstrate the higher conditioning capability of the proposed method in comparison with the tradi- tional algorithms. Finally, novel techniques on modeling non-stationary phenomena are introduced. The traditional approaches rely mainly on auxiliary variables to force the MPS algorithm into generating the desired spatial behavior; such as defining regions, constraining the facies proportions, or specifying the rotation/scaling of the features spatially. Rather in this thesis, the original MPS modeling paradigm, where a training image is the sole prerequisite for simulation, is re-established. The proposed framework conceptually embeds the spatial components of the patterns into geostatistical modeling. Various training images are used to demonstrate the capabilities of proposed approaches for modeling non-stationarity},
author = {Honarkhah, Mehrdad},
file = {:C$\backslash$:/Users/deric/Downloads/SCRF2009{\_}03.Mehrdad{\_}Honarkhah{\_}SCRF2009{\_}Final.pdf:pdf},
journal = {Mathematical Geosciences},
number = {May},
pages = {361},
title = {{Stochastic simulation of patterns using distance-based patten Modeling}},
volume = {42},
year = {2011}
}
@article{Shi2000,
abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph},
archivePrefix = {arXiv},
arxivId = {cs/0703101v1},
author = {Shi, Jianbo and Malik, Jitendra},
doi = {10.1109/34.868688},
eprint = {0703101v1},
file = {:C$\backslash$:/Users/deric/Downloads/Shi-Malik-CVPR-1997.pdf:pdf},
isbn = {0-8186-7822-4},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {computer vision,dissimilarity,eigenvalues,eigenvalues and eigenfunctions,graph,graph partitioning,grouping,image segmentation,image sequences,normalized cut,perceptual grouping,similarity,theory},
number = {8},
pages = {888--905},
pmid = {15742889},
primaryClass = {cs},
title = {{Normalized cuts and image segmentation}},
volume = {22},
year = {2000}
}
@inproceedings{weiss1999segmentation,
author = {Weiss, Yair},
booktitle = {Computer vision, 1999. The proceedings of the seventh IEEE international conference on},
file = {:C$\backslash$:/Users/deric/Downloads/iccv99.pdf:pdf},
organization = {IEEE},
pages = {975--982},
title = {{Segmentation using eigenvectors: a unifying view}},
volume = {2},
year = {1999}
}
@article{Calinski1974,
abstract = {A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek et al. (1951a), is constructed on a nearest neighbour basis ana then divided in to clusters by applying the criterion of minimum within-cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the “best number” of clusters is suggested. It is a “variance ratio criterion” giving some insight into the structure of the pointa. The method is illustrated by three examples, one of which is original. The results obtained bg the dendrite method are compared with those obtained by using the agglomerative method of 'Nard (1963) and the divisive method of Edwards and Cavalli-Sforza (1965). {\textcopyright} 1974, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Cali{\~{n}}ski, T. and Harabasz, J.},
doi = {10.1080/03610927408827101},
file = {:C$\backslash$:/Users/deric/Downloads/03610927408827101.pdf:pdf},
isbn = {2255782138},
issn = {00903272},
journal = {Communications in Statistics},
keywords = {approximate grouping procedure,cluster analysis,minimum variance (WGSS) criterion for optimal grouping,numerical taxonomy,shortest dendrite = minimum spanning tree,variance ratio criterion for best number of groups},
number = {1},
pages = {1--27},
title = {{A Dendrite Method Foe Cluster Analysis}},
volume = {3},
year = {1974}
}
@article{Rosenberger2008,
abstract = {We propose in this paper a face authentication method based on a similarity measure. The SIFT descriptor is used to define some interest keypoints characterized by an invariant parameter. A graph is then built where nodes correspond to these keypoints. We model the authentication problem as a graph matching process. Experimental results on the AR database show an EER equals to 12{\%} with only one image used for the enrollment and with images simulating real conditions.},
author = {Rosenberger, C and Brun, L},
doi = {10.1109/ICPR.2008.4761860},
file = {:C$\backslash$:/Users/deric/Downloads/04761860.pdf:pdf},
isbn = {978-1-4244-2174-9},
issn = {1051-4651},
journal = {Pattern Recognition, 2008. ICPR 2008. 19th International Conference on},
keywords = {AR database;SIFT descriptor;face authentication me},
pages = {1--4},
title = {{Similarity-based matching for face authentication}},
year = {2008}
}
@article{Iam-on2010,
abstract = {Cluster ensembles have emerged as a powerful meta-learning paradigm that provides improved accuracy and robustness by aggregating several input data clusterings. In par- ticular, link-based similarity methods have recently been introduced with superior per- formance to the conventional co-association approach. This paper presents a MATLAB package, LinkCluE, that implements the link-based cluster ensemble framework. A variety of functional methods for evaluating clustering results, based on both internal and exter- nal criteria, are also provided. Additionally, the underlying algorithms together with the sample uses of the package with interesting real and synthetic datasets are demonstrated herein.},
author = {Iam-on, Nattakan and Garrett, Simon},
doi = {http://dx.doi.org/10.18637/jss.v036.i09},
file = {:C$\backslash$:/Users/deric/Downloads/v36i09.pdf:pdf},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {cluster ensembles,cluster relation,clustering,link anal,pairwise similarity matrix},
number = {9},
pages = {1--36},
title = {{LinkCluE : A MATLAB Package for Link-Based}},
url = {http://www.jstatsoft.org/v36/i09/paper},
volume = {36},
year = {2010}
}
@article{Kassambara2015,
abstract = {Edition 1. "Multivariate analysis I"--Cover.},
author = {Kassambara, Alboukadel},
file = {:D$\backslash$:/octave/BOOKS{\_}Clustering{\_}AnalysiS/[Alboukadel Kassambara]{\_}Practical Guide to Cluster Analysis in R. Unsupervised Machine Learning.pdf:pdf},
isbn = {1542462703},
pages = {187},
title = {{Practical guide to cluster analysis in R : unsupervised machine learning}},
year = {2015}
}
@article{Davies1979,
abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
author = {Davies, David L. and Bouldin, Donald W.},
doi = {10.1109/TPAMI.1979.4766909},
file = {:C$\backslash$:/Users/deric/Downloads/04766909.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Index Terms-Cluster,data partitions,multidimensional data analysis,parametric clustering,partitions,similarity measure},
number = {2},
pages = {224--227},
pmid = {21868852},
title = {{A Cluster Separation Measure}},
volume = {PAMI-1},
year = {1979}
}
@article{Huband2005,
abstract = {Assessment of clustering tendency is an important first step in cluster analysis. One tool for assessing cluster tendency is the Visual Assessment of Tendency (VAT) algorithm. VAT produces an image matrix that can be used for visual assessment of cluster tendency in either relational or object data. However, VAT becomes intractable for large data sets. The revised VAT (reVAT) algorithm reduces the number of computations done by VAT, and replaces the image matrix with a set of profile graphs that are used for the visual assessment step. Thus, reVAT overcomes the large data set problem which encumbers VAT, but presents a new problem: interpretation of the set of reVAT profile graphs becomes very difficult when the number of clusters is large, or there is significant overlap between groups of objects in the data. In this paper, we propose a new algorithm called bigVAT which (i) solves the large data problem suffered by VAT, and (ii) solves the interpretation problem suffered by reVAT. bigVAT combines the quasi-ordering technique used by reVAT with an image display of the set of profile graphs displaying the clustering tendency information with a VAT-like image. Several numerical examples are given to illustrate and support the new technique. {\textcopyright} 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {Huband, Jacalyn M. and Bezdek, James C. and Hathaway, Richard J.},
doi = {10.1016/j.patcog.2005.03.018},
file = {:C$\backslash$:/Users/deric/Downloads/1-s2.0-S0031320305001408-main.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Cluster analysis,Cluster tendency,Visual assessment},
number = {11},
pages = {1875--1886},
title = {{BigVAT: Visual assessment of cluster tendency for large data sets}},
volume = {38},
year = {2005}
}
@article{Sun2015,
author = {Sun, Weiwei and Zhang, Liangpei and Member, Senior and Du, Bo and Member, Senior and Li, Weiyue and Lai, Yenming Mark},
file = {:C$\backslash$:/Users/deric/Downloads/07085932.pdf:pdf},
number = {6},
pages = {2784--2797},
title = {{Clustering for Hyperspectral Imagery Classification}},
volume = {8},
year = {2015}
}
@article{Navarro1996,
abstract = {We use high-resolution N-body simulations to study the equilibrium density profiles of dark matter halos in hierarchically clustering universes. We find that all such profiles have the same shape, independent of halo mass, of initial density fluctuation spectrum, and of the values of the cosmological parameters. Spherically averaged equilibrium profiles are well fit over two decades in radius by a simple formula originally proposed to describe the structure of galaxy clusters in a cold dark matter universe. In any particular cosmology the two scale parameters of the fit, the halo mass and its characteristic density, are strongly correlated. Low-mass halos are significantly denser than more massive systems, a correlation which reflects the higher collapse redshift of small halos. The characteristic density of an equilibrium halo is proportional to the density of the universe at the time it was assembled. A suitable definition of this assembly time allows the same proportionality constant to be used for all the cosmologies that we have tested. We compare our results to previous work on halo density profiles and show that there is good agreement. We also provide a step-by-step analytic procedure, based on the Press-Schechter formalism, which allows accurate equilibrium profiles to be calculated as a function of mass in any hierarchical model.},
archivePrefix = {arXiv},
arxivId = {astro-ph/9611107},
author = {Navarro, Julio F. and Frenk, Carlos S. and White, Simon D. M.},
doi = {10.1086/304888},
eprint = {9611107},
file = {:C$\backslash$:/Users/deric/Downloads/Navarro{\_}1997{\_}ApJ{\_}490{\_}493.pdf:pdf},
isbn = {0004-637X},
issn = {0004-637X},
number = {1},
pages = {493--508},
primaryClass = {astro-ph},
title = {{A Universal Density Profile from Hierarchical Clustering}},
url = {http://arxiv.org/abs/astro-ph/9611107{\%}0Ahttp://dx.doi.org/10.1086/304888},
volume = {1},
year = {1996}
}
@article{Gan1988,
author = {Gan, Guojun and Ma, Chaoqum and Wu, Jianhong},
file = {:C$\backslash$:/Users/deric/Downloads/gan2007toc.pdf:pdf},
pages = {455},
title = {{Data clustering: Theory, algorithms, and applications}},
year = {1988}
}
@article{Author,
abstract = {The abstract is a mandatory element that should summarize the con-tents of the paper and should contain 15-250 words. Abstract and keywords are made freely available in SpringerLink.},
author = {Author, First and Author, Second and Author, Third and Author, Fourth},
keywords = {by,capitalized,each keyword should be,here,if possible,middots,please list your keywords,the first letter of,they should be separated},
pages = {1--11},
title = {{Springer Guidelines for Authors of Proceedings}}
}
@article{Li2014,
abstract = {With the development of hyperspectral remote sensing technology, the spectral resolution of the hyperspectral image data becomes denser, which results in large number of bands, high correlation between neighboring bands, and high data redundancy. It is necessary to reduce these bands before further analysis, such as land cover classification and target detection. Aiming at the classification task, this paper proposes an effective band selection method from the novel perspective of spectral shape similarity analysis with key points extraction and thus retains physical information of hyperspectral remote sensing images. The proposed approach takes all the bands of hyperspectral remote sensing images as time series. Firstly, spectral clustering is utilized to cluster all the training samples, which produces the prototypical spectral curves of each cluster. Then a set of initial candidate bands are obtained based on the extraction of key points from the processed hyperspectral curves, which preserve discriminative information and narrow down the candidate band subset for the following search procedure. Finally, filtering contiguous bands according to conditional mutual information and branch and bound search are further performed sequentially to gain the optimal band combination. To verify the effectiveness of the integrated band selection method put forward in this paper, classification employing the Support Vector Machine (SVM) classifier is performed on the selected spectral bands. The experimental results on two publicly available benchmark data sets demonstrate that the presented approach can select those bands with discriminative information, usually about 10 out of 200 original bands. Compared with previous studies, the newly proposed method is competitive with far fewer bands selected and a lower computational complexity, while the classification accuracy remains comparable. {\textcopyright}2013 The Authors.},
author = {Li, Shijin and Qiu, Jianbin and Yang, Xinxin and Liu, Huan and Wan, Dingsheng and Zhu, Yuelong},
doi = {10.1016/j.engappai.2013.07.010},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Branch and bound search,Conditional mutual information,Feature selection,Hyperspectral remote sensing,Key point of time series,Spectral clustering},
pages = {241--250},
publisher = {Elsevier},
title = {{A novel approach to hyperspectral band selection based on spectral shape similarity analysis and fast branch and bound search}},
url = {http://dx.doi.org/10.1016/j.engappai.2013.07.010},
volume = {27},
year = {2014}
}
@article{Captions2016,
author = {Captions, Their and Reading, Further and Index, Subject},
pages = {1--13},
title = {{A T X 2 SVProc Document Class L E {\$}\epsilon{\$} Author Instructions for Step-by-Step Instructions}},
year = {2016}
}
@article{gan2014investigation,
author = {Gan, Sonny and Cosgrove, David A and Gardiner, Eleanor J and Gillet, Valerie J},
journal = {Journal of chemical information and modeling},
number = {12},
pages = {3302--3319},
publisher = {ACS Publications},
title = {{Investigation of the use of spectral clustering for the analysis of molecular data}},
volume = {54},
year = {2014}
}
@article{Ng2002,
abstract = {Despite many empirical successes of spectral clustering methods -- algorithms that clusters points using eigenvectors of matrices derived from the data -- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
author = {Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
doi = {10.1.1.19.8100},
isbn = {0818619155},
issn = {{\textless}null{\textgreater}},
journal = {Advances in Neural Information Processing Systems 14},
pages = {849--856},
title = {{On Spectral Clustering: Analysis and an Algorithm}},
url = {http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf},
year = {2002}
}
@article{weller2015survey,
author = {Weller-Fahy, David J and Borghetti, Brett J and Sodemann, Angela A},
journal = {IEEE Communications Surveys {\&} Tutorials},
number = {1},
pages = {70--91},
publisher = {IEEE},
title = {{A survey of distance and similarity measures used within network intrusion anomaly detection}},
volume = {17},
year = {2015}
}
@inproceedings{von2004convergence,
author = {{Von Luxburg}, Ulrike and Bousquet, Olivier and Belkin, Mikhail},
booktitle = {International Conference on Computational Learning Theory},
organization = {Springer},
pages = {457--471},
title = {{On the convergence of spectral clustering on random samples: the normalized case}},
year = {2004}
}
@article{shi2000normalized,
author = {Shi, Jianbo and Malik, Jitendra},
journal = {IEEE Transactions on pattern analysis and machine intelligence},
number = {8},
pages = {888--905},
publisher = {Ieee},
title = {{Normalized cuts and image segmentation}},
volume = {22},
year = {2000}
}
